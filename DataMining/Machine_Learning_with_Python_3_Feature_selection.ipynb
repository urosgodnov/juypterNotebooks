{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2dFGyGUXqLW5szSGH/EwF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urosgodnov/juypterNotebooks/blob/main/DataMining/Machine_Learning_with_Python_3_Feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K028aoI3dM0U"
      },
      "source": [
        "**Using Python to implement machine learning process\n",
        "by dr. Uros Godnov**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhH3-abDiNhL"
      },
      "source": [
        "# Feature selection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Filter Methods:**\n",
        "\n",
        "Filter methods are used to score each feature independently of the model and then select the highest scoring features:\n",
        "- Variance Threshold\n",
        "- Correlation Matrix with Heatmap\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "2. **Wrapper Methods:**\n",
        "\n",
        "Wrapper methods involve training a machine learning model to evaluate the importance of features, where the model's performance serves as a criterion:\n",
        "\n",
        "- Recursive Feature Elimination (RFE)\n",
        "- Sequential Feature Selection\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "3. **Embedded Methods**\n",
        "\n",
        "Embedded methods are techniques that are performed during model training and can provide feature importance scores directly from the algorithm:\n",
        "\n",
        "- Feature Importance with Tree-Based Models\n",
        "- L1 Regularization (Lasso)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "4. **Principal Component Analysis (PCA)**\n",
        "\n",
        "PCA is a dimensionality reduction technique that transforms features into a lower-dimensional space. While it doesn’t technically “select” features, it can help reduce feature dimensionality."
      ],
      "metadata": {
        "id": "mDo7P0-pTtgE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfYCyJZRiXfV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "import sys\n",
        "# Mount google drive\n",
        "drive.mount('/content/gdrive')\n",
        "# Changing path dirctory\n",
        "sys.path.append('/content/gdrive/MyDrive/Google_Colab_modules')\n",
        "\n",
        "import sweetviz as sv\n",
        "import ydata_profiling as ydp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oB_HDRV6kMN4"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv(\"https://raw.githubusercontent.com/urosgodnov/datasets/refs/heads/master/laptip_prices_without_missing_values.csv\")\n",
        "for col in df.select_dtypes(exclude=\"number\"):\n",
        "    # Count frequencies including NaN\n",
        "    counts = df[col].value_counts(dropna=False)\n",
        "\n",
        "    # Replace categories with frequency ≤ 65, but keep NaNs\n",
        "    df[col] = df[col].apply(\n",
        "        lambda x: x if pd.isnull(x) or counts[x] > 65 else \"Other\"\n",
        "    )\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation matrix"
      ],
      "metadata": {
        "id": "bhiAy-36PdVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.select_dtypes(include=['number','float']).corr()"
      ],
      "metadata": {
        "id": "6C336W2tPgRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**No** p- values.\n",
        "What is a p-value?\n",
        "\n",
        "Key points about p-values:\n",
        "- Low p-value (e.g., < 0.05): Suggests that the observed data is unlikely under the null hypothesis, leading to rejection of the null hypothesis in favor of the alternative hypothesis. This implies the result is statistically significant.\n",
        "\n",
        "- High p-value (e.g., > 0.05): Indicates that the observed data is consistent with the null hypothesis, meaning there is not enough evidence to reject it. The result is considered not statistically significant.\n",
        "\n",
        "Interpretation:\n",
        "- p < 0.01: Strong evidence against the null hypothesis.\n",
        "- p < 0.05: Moderate evidence against the null hypothesis (common threshold for significance).\n",
        "- p > 0.05: Weak evidence against the null hypothesis; you may not reject the null.\n",
        "\n",
        "While p-values are useful for determining statistical significance, they don't measure the size or importance of an effect and can be affected by sample size. Therefore, it's crucial to interpret them in the context of the study and other statistics (e.g., confidence intervals)."
      ],
      "metadata": {
        "id": "DOreOkNTjrGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "dfC=df.select_dtypes(include=['number','float'])\n",
        "\n",
        "correlation_matrix = dfC.corr()\n",
        "p_values = pd.DataFrame(index=dfC.columns, columns=dfC.columns)\n",
        "\n",
        "for col1 in dfC.columns:\n",
        "    for col2 in dfC.columns:\n",
        "        if col1 == col2:\n",
        "            p_values.loc[col1, col2] = None  # p-value for self correlation is not meaningful\n",
        "        else:\n",
        "            corr, p_val = pearsonr(df[col1], df[col2])\n",
        "            p_values.loc[col1, col2] = p_val\n",
        "\n",
        "print(\"Correlation Matrix:\")\n",
        "print(correlation_matrix)\n",
        "print(\"\\nP-Values Matrix:\")\n",
        "print(p_values)"
      ],
      "metadata": {
        "id": "jtIB-w1VQzFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Strongest correlation: ram_gb has the strongest relationship with price_euro (correlation = 0.7400), and this relationship is highly significant.\n",
        "- Moderate correlation: cpu_frequency_ghz shows a moderate correlation with price.\n",
        "- Weak correlations: inches and weight_kg have weak but statistically significant correlations with price_euro."
      ],
      "metadata": {
        "id": "phTIDaV6lIrk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XC0zv5IET32n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## T test and ANOVA"
      ],
      "metadata": {
        "id": "AH2ywElGlvNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Assumptions** (t test):\n",
        "- The two groups are independent (no overlap).\n",
        "- The data is approximately normally distributed.\n",
        "- Variances between the two groups are equal (homogeneity of variance). If not, a Welch's t-test can be used as a variant.\n",
        "\n",
        "**Key Assumptions** (ANOVA):\n",
        "- ANOVA is used when comparing the means of three or more groups.\n",
        "- It determines if at least one group has a significantly different mean without performing multiple pairwise t-tests, which can increase the risk of Type I errors (false positives)."
      ],
      "metadata": {
        "id": "FDoFWkQBlrpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy import stats\n",
        "\n",
        "# Identify object-type columns\n",
        "object_columns = df.select_dtypes(include='object').columns\n",
        "\n",
        "# Iterate through each object-type column\n",
        "for col in object_columns:\n",
        "    # Drop rows with NaN in the current column or 'price_euro'\n",
        "    df_clean = df.dropna(subset=[col, 'price_euro'])\n",
        "    unique_values = df_clean[col].unique()\n",
        "\n",
        "    if len(unique_values) == 2:\n",
        "        # Perform t-test for two unique values\n",
        "        group1 = df_clean[df_clean[col] == unique_values[0]]['price_euro']\n",
        "        group2 = df_clean[df_clean[col] == unique_values[1]]['price_euro']\n",
        "\n",
        "        if len(group1) >= 2 and len(group2) >= 2:\n",
        "            # Perform independent t-test\n",
        "            t_stat, p_value = stats.ttest_ind(group1, group2, equal_var=False)  # Welch's t-test\n",
        "\n",
        "            print(f\"T-test for column '{col}', comparing '{unique_values[0]}' vs '{unique_values[1]}':\")\n",
        "            print(f\"t-statistic: {t_stat:.4f}, p-value: {p_value:.4e}\\n\")\n",
        "        else:\n",
        "            print(f\"Not enough data in one of the groups for column '{col}'.\\n\")\n",
        "\n",
        "    elif len(unique_values) >= 3:\n",
        "        # Perform ANOVA for three or more unique values\n",
        "        groups = [df_clean[df_clean[col] == value]['price_euro'] for value in unique_values]\n",
        "\n",
        "        # Check if all groups have at least two observations\n",
        "        if all(len(group) >= 2 for group in groups):\n",
        "            # Perform one-way ANOVA\n",
        "            f_stat, p_value = stats.f_oneway(*groups)\n",
        "\n",
        "            print(f\"ANOVA for column '{col}':\")\n",
        "            print(f\"F-statistic: {f_stat:.4f}, p-value: {p_value:.4e}\\n\")\n",
        "        else:\n",
        "            print(f\"Not enough data in one of the groups for column '{col}'.\\n\")\n"
      ],
      "metadata": {
        "id": "xaAc7Pfjl1K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regular linear regression"
      ],
      "metadata": {
        "id": "0ioEmzCtDL3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for the purpose of demonstration, we select only numeric columns\n",
        "# importing stats model - OLS regression\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "df_numeric = df.select_dtypes(include=[np.number])\n",
        "\n",
        "# Let's say we want to predict 'Y' using all other numeric columns.\n",
        "dependent_var = 'price_euro'\n",
        "independent_vars = df_numeric.columns.drop(dependent_var)  # all numeric cols except 'Y'\n",
        "\n",
        "# 3. Construct the formula string for statsmodels\n",
        "formula = f\"{dependent_var} ~ {' + '.join(independent_vars)}\"\n",
        "print(\"Formula:\", formula)\n",
        "# e.g., \"Y ~ X1 + X2\"\n",
        "\n",
        "# 4. Fit the linear regression model\n",
        "model = ols(formula, data=df_numeric).fit()"
      ],
      "metadata": {
        "id": "WFNO9odxDRle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "tYeY3afhEUjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scikit feature selection"
      ],
      "metadata": {
        "id": "fNTF-mc6p8lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SelectKBest with f_regression"
      ],
      "metadata": {
        "id": "h1gHgGa7qhyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "f_regression is a function from the sklearn.feature_selection module that evaluates the linear relationship between each feature (independent variable) and the target (dependent variable) using an F-test in a regression setting.\n",
        "\n",
        "**Compare it to previous linear regression model!**"
      ],
      "metadata": {
        "id": "-sZYU2fsFDd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_regression\n"
      ],
      "metadata": {
        "id": "o7Lfvje_p_rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOMINAL variables - get_dummies()**\n",
        "\n"
      ],
      "metadata": {
        "id": "dGEKZ-3RFe0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xdemo = pd.DataFrame({\n",
        "    'Color': ['Red', 'Blue', 'Green'],\n",
        "    'Size': ['S', 'M', 'L'],\n",
        "    'Price': [10, 15, 20]\n",
        "})\n",
        "\n",
        "print(Xdemo)\n",
        "Xdemo_dummies = pd.get_dummies(Xdemo)\n",
        "print(Xdemo_dummies)"
      ],
      "metadata": {
        "id": "ReZsyI3hFua2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['price_euro']\n",
        "\n",
        "X = df.drop('price_euro', axis=1)\n",
        "\n",
        "## get_dummies is similiar to one-code\n",
        "X_dummies = pd.get_dummies(X)\n",
        "X_dummies.head()"
      ],
      "metadata": {
        "id": "JklpVKU7qB7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_dummies)\n",
        "\n",
        "X_scaled"
      ],
      "metadata": {
        "id": "l-ykAHjgq3aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selector = SelectKBest(score_func=f_regression, k=10)  # Select top 10 features\n",
        "\n",
        "# Fit the selector to the data\n",
        "X_new = selector.fit_transform(X_scaled, y)\n",
        "\n",
        "# Get the boolean mask of selected features\n",
        "mask = selector.get_support()\n",
        "\n",
        "# Get the list of selected feature names\n",
        "selected_features = X_dummies.columns[mask]\n",
        "\n",
        "selected_features"
      ],
      "metadata": {
        "id": "40VK8jF-rYCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cheating with ordinal coding"
      ],
      "metadata": {
        "id": "20sScXc6tOyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "categorical_cols=df.select_dtypes(include='object').columns\n",
        "\n",
        "ordinal_encoder = OrdinalEncoder()\n",
        "\n",
        "X_encoded = X.copy()\n",
        "\n",
        "X_encoded[categorical_cols] = ordinal_encoder.fit_transform(X[categorical_cols])\n",
        "\n",
        "X_encoded.transpose()"
      ],
      "metadata": {
        "id": "Upy2E4OctS3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if we want to see a mapping values\n",
        "# Map encoded integers back to original categories\n",
        "print(\"\\nMapping of Encoded Values to Original Categories:\")\n",
        "for col, categories in zip(categorical_cols, ordinal_encoder.categories_):\n",
        "    print(f\"{col}: {dict(enumerate(categories))}\")"
      ],
      "metadata": {
        "id": "ysyjuo6PKLp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selector = SelectKBest(score_func=f_regression, k=7)\n",
        "\n",
        "X_new = selector.fit_transform(X_encoded, y)\n",
        "\n",
        "\n",
        "# Get the boolean mask of selected features\n",
        "mask = selector.get_support()\n",
        "\n",
        "# Get the list of selected feature names\n",
        "selected_features = X.columns[mask]\n",
        "\n",
        "selected_features"
      ],
      "metadata": {
        "id": "AQyT91Bgt6q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature importance**"
      ],
      "metadata": {
        "id": "Rjt829EIvSFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = selector.scores_\n",
        "pvalues = selector.pvalues_\n",
        "\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X_encoded.columns,  # The original feature names after encoding\n",
        "    'F-Score': scores,\n",
        "    'p-Value': pvalues\n",
        "})\n",
        "\n",
        "# Sort by F-score to display the most important features\n",
        "feature_importances_sorted = feature_importances.sort_values(by='F-Score', ascending=False)\n",
        "feature_importances_sorted.head(7)"
      ],
      "metadata": {
        "id": "IHp-PeMhvUDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a pipeline"
      ],
      "metadata": {
        "id": "kdr8eNZKz-gg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a pipeline with Scikit-learn has several advantages, particularly in machine learning workflows. Here’s why you should use a pipeline:\n",
        "\n",
        "- Streamlines Workflow: Combines multiple steps like preprocessing, feature selection, and model training into a single object, making the workflow more organized and easier to manage.\n",
        "\n",
        "- Prevents Data Leakage: Ensures that all transformations (like scaling or encoding) are applied only on training data during cross-validation, avoiding the use of information from the test set in training.\n",
        "\n",
        "- Consistency: Ensures that the same transformations are applied to both the training and test datasets, maintaining uniformity throughout the machine learning process.\n",
        "\n",
        "- Cleaner Code: Simplifies code by chaining all preprocessing and modeling steps, reducing the need to manually apply transformations for every step.\n",
        "\n",
        "- Cross-Validation Compatibility: Allows seamless use of cross-validation (cross_val_score, GridSearchCV, etc.) by treating the entire pipeline as a single entity, making it easy to optimize hyperparameters across all steps.\n",
        "\n",
        "- Reusable: The pipeline can be reused for both training and prediction, applying all transformations automatically to any new data.\n",
        "\n",
        "- Modular Design: Each step in the pipeline is modular, making it easier to update or replace specific parts (e.g., swapping out a model or a preprocessing step) without affecting the entire workflow.\n",
        "\n",
        "- Reduces Human Error: By automating data transformations within the pipeline, it reduces the chance of making mistakes (like forgetting to scale test data) when manually performing steps"
      ],
      "metadata": {
        "id": "KIPpR4mE0SeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n"
      ],
      "metadata": {
        "id": "a4Dk4Ip50obK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pipeline = Pipeline(steps=[\n",
        "#    ('step_name', transformer_or_model),\n",
        "#    ('step_name2', transformer_or_model2),\n",
        "#    # Add more steps as needed\n",
        "# ])"
      ],
      "metadata": {
        "id": "fLpXAmxn1ejS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipeline for preprocessing"
      ],
      "metadata": {
        "id": "YRPY8tQZ01va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "dir(sklearn.feature_selection)"
      ],
      "metadata": {
        "id": "j7iq6zKCSL3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SelectKBest"
      ],
      "metadata": {
        "id": "v_mMYehL3_0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data importing and frequency recalculation\n",
        "\n",
        "df=pd.read_csv(\"https://raw.githubusercontent.com/urosgodnov/datasets/refs/heads/master/laptip_prices_without_missing_values.csv\")\n",
        "\n",
        "for col in df.select_dtypes(exclude=\"number\"):\n",
        "    # Count frequencies including NaN\n",
        "    counts = df[col].value_counts(dropna=False)\n",
        "\n",
        "    # Replace categories with frequency ≤ 65, but keep NaNs\n",
        "    df[col] = df[col].apply(\n",
        "        lambda x: x if pd.isnull(x) or counts[x] > 65 else \"Other\"\n",
        "    )"
      ],
      "metadata": {
        "id": "Rd8EvKaGQAk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# independent and dependent variables\n",
        "X = df.drop('price_euro', axis=1)\n",
        "y = df['price_euro']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()"
      ],
      "metadata": {
        "id": "xBmJFLZl05_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "- transformers argument:\n",
        " - ('num', StandardScaler(), numerical_cols):\n",
        "   - Apply StandardScaler to the columns in numerical_cols.\n",
        "   - StandardScaler standardizes numerical features to have zero mean and unit variance.\n",
        " - ('cat', OrdinalEncoder(), categorical_cols):\n",
        "   - Apply OrdinalEncoder to the columns in categorical_cols.\n",
        "   - OrdinalEncoder converts categorical values into integers based on their order (e.g., ['Low', 'Medium', 'High'] → [0, 1, 2]).\n",
        "\n",
        "- Each tuple in transformers specifies:\n",
        " - A name for the transformation ('num' or 'cat').\n",
        " - The transformation object (StandardScaler or OrdinalEncoder).\n",
        " - The columns to which the transformation will be applied (numerical_cols or categorical_cols)."
      ],
      "metadata": {
        "id": "eD1JQY5yL090"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# defining preprocessor\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OrdinalEncoder(), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# defining pipline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing step\n",
        "    ('feature_selection', SelectKBest(score_func=f_classif, k=7))  # Feature selection step\n",
        "])"
      ],
      "metadata": {
        "id": "3F09WNVS2Juo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X, y)\n",
        "\n",
        "X_selected = pipeline.transform(X)\n",
        "\n",
        "selector = pipeline.named_steps['feature_selection']\n",
        "\n",
        "scores = selector.scores_  # F-scores\n",
        "pvalues = selector.pvalues_  # p-values\n",
        "\n",
        "mask = selector.get_support()\n",
        "\n",
        "selected_features = X.columns[mask]\n",
        "\n",
        "# Output the selected feature names\n",
        "print(\"Selected Features:\", selected_features)"
      ],
      "metadata": {
        "id": "KD23a47D26oM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances_selectkbest = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'F-Score': scores,\n",
        "    'p-Value': pvalues\n",
        "})\n",
        "\n",
        "feature_importances_selectkbest = feature_importances_selectkbest[feature_importances_selectkbest['Feature'].isin(selected_features)].sort_values(by='F-Score', ascending=False)\n",
        "feature_importances_selectkbest.head()"
      ],
      "metadata": {
        "id": "txHi5dPs6Quk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variance thresholder"
      ],
      "metadata": {
        "id": "vcJovrDCUJ1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing step\n",
        "    ('feature_selection', VarianceThreshold(threshold=0.8))\n",
        "])"
      ],
      "metadata": {
        "id": "-_xkhBqfUW60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X, y)\n",
        "\n",
        "X_selected = pipeline.transform(X)\n",
        "\n",
        "selector = pipeline.named_steps['feature_selection']\n",
        "\n",
        "\n",
        "mask = selector.get_support()\n",
        "variances = selector.variances_\n",
        "\n",
        "selected_features = X.columns[mask]\n",
        "\n",
        "# Output the selected feature names\n",
        "print(\"Selected Features:\", selected_features)"
      ],
      "metadata": {
        "id": "q6frjkBLU6UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances_variance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Variance': variances\n",
        "})\n",
        "\n",
        "feature_importances_variance[feature_importances_variance[\"Variance\"]>0.5]"
      ],
      "metadata": {
        "id": "ZUE0fNNiVdss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recursive Feature Elimination (RFE)\n",
        "\n",
        "Recursive Feature Elimination (RFE) is a feature selection technique used to identify the most important features for a predictive model. It works by recursively removing the least important features and fitting the model again on the remaining features."
      ],
      "metadata": {
        "id": "169d1UP64M-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing step\n",
        "    ('feature_selection', RFE(estimator=LinearRegression(), n_features_to_select=7))  # Feature selection step\n",
        "])\n"
      ],
      "metadata": {
        "id": "HpI8lbH84l7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X, y)\n",
        "\n",
        "X_selected = pipeline.transform(X)\n",
        "\n",
        "selector = pipeline.named_steps['feature_selection']\n",
        "\n",
        "ranking = selector.ranking_\n",
        "support = selector.support_\n",
        "\n",
        "mask = selector.get_support()\n",
        "\n",
        "selected_features = X.columns[mask]\n",
        "\n",
        "# Output the selected feature names\n",
        "print(\"Selected Features:\", selected_features)"
      ],
      "metadata": {
        "id": "DK5w3chm47Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importances_RFE = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Ranking': ranking,\n",
        "    'Selected': support\n",
        "})\n",
        "\n",
        "feature_importances_RFE = feature_importances_RFE[feature_importances_RFE['Feature'].isin(selected_features)].sort_values(by='Ranking')\n",
        "feature_importances_RFE.head()"
      ],
      "metadata": {
        "id": "UBXhoDfY6m01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balancing data"
      ],
      "metadata": {
        "id": "z_jAKEzxb3fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balancing:\n",
        "\n",
        "- Oversampling: SMOTE, ADASYN, RandomOverSampler\n",
        "- Undersampling: RandomUnderSampler, TomekLinks\n",
        "- Combination: SMOTEENN, SMOTETomek\n",
        "\n",
        "**scikit-learn's Pipeline doesn't support resampling steps like SMOTE**.\n",
        "\n",
        "imbalanced-learn (imblearn) provides a compatible Pipeline that seamlessly integrates balancing techniques."
      ],
      "metadata": {
        "id": "-aSsX0bKb7tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use SelectKBest(score_func=f_classif, k=7) to find 5 feature\n",
        "# neglect data quality\n",
        "df_task=pd.read_csv(\"https://raw.githubusercontent.com/urosgodnov/datasets/refs/heads/master/diabetes.csv\")\n",
        "\n",
        "df_task.head()"
      ],
      "metadata": {
        "id": "GOJgFX_MZVhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# frequncy od dependent variable\n",
        "df_task['Class_variable'].value_counts()"
      ],
      "metadata": {
        "id": "VI2z3HbPcGEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using combination of SMOTE and RandomUnderSampler\n",
        "# SMOTE: Generates synthetic samples for the minority class.\n",
        "# RandomUnderSampler: Reduces the number of samples in the majority class.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "X = df_task.drop('Class_variable', axis=1)\n",
        "y = df_task['Class_variable']\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Display class distribution in training set\n",
        "print(\"\\nTraining set class distribution:\")\n",
        "print(y_train.value_counts())"
      ],
      "metadata": {
        "id": "EAcEj23CdBBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "numerical_cols"
      ],
      "metadata": {
        "id": "XhOgzA86eUMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep other columns unchanged\n",
        ")"
      ],
      "metadata": {
        "id": "f7ruh1Qjenzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the resampling strategy\n",
        "smote = SMOTE(random_state=42, sampling_strategy=0.8)  # Over-sample minority class\n",
        "under = RandomUnderSampler(random_state=42, sampling_strategy='majority')  # Under-sample majority class"
      ],
      "metadata": {
        "id": "5sMzCZdjer3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_selector = SelectKBest(score_func=f_classif, k=3)"
      ],
      "metadata": {
        "id": "KiWZk70ne8l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot original class distribution\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=y_train)\n",
        "plt.title('Original Training Set Class Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nNhndmIwfyjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After resampling\n",
        "resampling_pipeline = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing step\n",
        "    ('smote', smote),                # Over-sampling step\n",
        "    ('under', under)                 # Under-sampling step\n",
        "])\n",
        "\n",
        "X_resampled, y_resampled = resampling_pipeline.fit_resample(X_train, y_train)\n",
        "\n",
        "# Plot resampled class distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=y_resampled)\n",
        "plt.title('Resampled Training Set Class Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fo_cLYMbgF-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),  # Preprocessing step\n",
        "    ('smote', smote),                # Over-sampling step\n",
        "    ('under', under),                # Under-sampling step\n",
        "    ('feature_selection', feature_selector)\n",
        "])"
      ],
      "metadata": {
        "id": "RaTN05S7eyYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "selector = pipeline.named_steps['feature_selection']\n",
        "\n",
        "scores = selector.scores_  # F-scores\n",
        "pvalues = selector.pvalues_  # p-values\n",
        "\n",
        "mask = selector.get_support()\n",
        "\n",
        "selected_features = X.columns[mask]\n",
        "\n",
        "print(selected_features)\n",
        "\n",
        "feature_importances_selectkbest = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'F-Score': scores,\n",
        "    'p-Value': pvalues\n",
        "})\n",
        "\n",
        "feature_importances_selectkbest.sort_values(by='F-Score', ascending=False).head()"
      ],
      "metadata": {
        "id": "jul25Fm4ilZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task"
      ],
      "metadata": {
        "id": "Ye0vJaX_k526"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get cancer dataset\n",
        "# do all steps until feature selection\n",
        "# check the distribution of target variable\n",
        "# which independent variable are the features?\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "breast_cancer = load_breast_cancer()\n",
        "\n",
        "# Convert the data to a DataFrame\n",
        "df_task = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "\n",
        "# Add the target variable to the DataFrame\n",
        "df_task['target'] = breast_cancer.target\n",
        "\n",
        "df_task.head()"
      ],
      "metadata": {
        "id": "E12HVbUSk8bR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}